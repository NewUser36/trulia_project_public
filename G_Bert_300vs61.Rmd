---
title: "G_bert300vs61"
output:
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
options(digits=4)
```

To really compare the predictive power of BERT based models, I only use the text as covariates in the models.

# BERT with PCA and 300 first principal components

```{r}
# global variables
home_directory <- "~/trulia_project"
setwd(home_directory)

set.seed(1)

# R packages
library(tidyverse); library(magrittr); library(survival); library(survminer); library(lubridate); library(tm); library(FactoMineR); library(factoextra); library(feather); library(riskRegression); library(pec); library(survAUC); library(glmnet); library(plotmo)
# magrittr : use %$%
# lubridate : %--%
# tm : tf-idf
# FactoMineR : PCA
# pec : pec, crps
# survAUC : predErr
# plotmo : plot_glmnet

# My functions
source("utils.R")
source('generate_embeddings.R')
'%not in%' <- Negate('%in%')

# glmnet hyperparameters
glmnet.control(mxitnr=50)

############
# Datasets #
############

df_tfidf_99 <- read_feather("embeddings/tfidf_99.feather")
df_ft <- read_feather("embeddings/fasttext.feather")
df_bbu <- read_feather("embeddings/bert_base_uncased.feather")

#################################################
# Removing extreme and influential observations #
#################################################
# Somes homes have been on sale for 7+ years, which I find highly unprobable
# Keeping them in the dataset changes significantly results. Considering that
# they are probably problematic observations, I prefer to remove them.
df_bbu <- df_bbu %>%
  subset(subset=!(lubridate::year(listed_date) <= 2014 & 
                   lubridate::year(sold_date) >= 2021)
        )
df_ft <- df_ft %>%
  subset(subset=!(lubridate::year(listed_date) <= 2014 & 
                   lubridate::year(sold_date) >= 2021)
        )

df_tfidf_99 <- df_tfidf_99 %>%
  subset(subset=!(lubridate::year(listed_date) <= 2014 & 
                   lubridate::year(sold_date) >= 2021)
        )

# when I saved df_bbu, I hadn't modified sale_time > 20, so I do it here
# It is necessary to merge datasets properly.
df_bbu <- df_bbu %>%
  mutate(
    to_modify = case_when(sale_time > 20 ~ 1
                          ,TRUE ~ 0 # else
                          )
    , event = case_when(to_modify == 1 ~ 0 # censoring events flagged as to_modify
                       ,TRUE ~ event)
    , sale_time = case_when(to_modify ==1 ~ 20 # sale_time forced to 20 months
                            ,TRUE ~ sale_time)
  )

# PCA
# generating PCA coordinated for bert embeddings
pca_bbu <- df_bbu %>%
  select(V1:V768) %>%
  PCA(., scale.unit = FALSE, graph = F, ncp=768)

# using 80% rule (will be used in coxph later)
#num_dimension <- length(as.data.frame(pca_bbu$eig)$'cumulative percentage of variance'[as.data.frame(pca_bbu$eig)$'cumulative percentage of variance' <= 80]) + 1 
num_dimension = 300

pca_bbu_coord <- data.frame(pca_bbu$ind$coord)
colnames(pca_bbu_coord) <- paste0("bbu_dim.", 1:768)
df_bbu_pca <- cbind(df_bbu, pca_bbu_coord)

# merging df
# good practice : decide by which variable we merge df. It is not necessary here
# but I leave the code for future reference
# df <- left_join(df_tfidf_99, df_ft, by = c("sold_date", "sold_price", "listed_date", "listed_price", "year_built", "num_bath", "num_bed", "living_area", "lot_area", "I_heating", "I_cooling", "I_type", "subtype", "I_parking", "I_outdoor", "I_pool", "num_restaurants", "num_groceries", "num_nightlife", "description", "description_cased", "event", "price_ratio")) %>%
#   left_join(., df_bbu_pca, by = c("sold_date", "sold_price", "listed_date", "listed_price", "year_built", "num_bath", "num_bed", "living_area", "lot_area", "I_heating", "I_cooling", "I_type", "subtype", "I_parking", "I_outdoor", "I_pool", "num_restaurants", "num_groceries", "num_nightlife", "description", "description_cased", "event", "price_ratio"))

df <- left_join(df_tfidf_99, df_ft) %>%
  left_join(., df_bbu_pca)

# Houses on sale for >20 months or that took more than 20 months to sell
# are censored to 20 months
# THIS CODE IS NOT NECESSARY ANYMORE because I did it in 1_exploration and I
# exported the dataset.
# df <- df %>%
#   mutate(
#     to_modify = case_when(sale_time > 20 ~ 1
#                           ,TRUE ~ 0 # else
#                           )
#     , event = case_when(to_modify == 1 ~ 0 # censoring events flagged as to_modify
#                        ,TRUE ~ event)
#     , sale_time = case_when(to_modify ==1 ~ 20 # sale_time forced to 20 months
#                             ,TRUE ~ sale_time)
#   )
# before censoring events that took too long, there were 1567 events. After
# censoring at 20 months, there where 1537 events (20% of the dataset).

# REMARK : 
# For the function extract_nonzero_coeff to work, variables treated as
# factors must be transformed as factor here
df <- df %>%
  mutate(
    # format baths, beds, lot_area, etc
     year_built = as.numeric(year_built)
    ,num_bath = as.numeric(num_bath)
    ,num_bed = as.numeric(num_bed)
    ,living_area = as.numeric(living_area)
    ,lot_area = as.numeric(lot_area)
    ,listed_date = lubridate::date(listed_date)
    ,sold_date = lubridate::date(sold_date)
    ,I_heating = as.factor(I_heating)
    ,I_cooling = as.factor(I_cooling)
    ,I_parking = as.factor(I_parking)
    ,I_outdoor = as.factor(I_outdoor)
    ,I_pool = as.factor(I_pool)
    ,I_type = as.factor(I_type)
    ,event = as.numeric(event)
  ) %>%
  # need to have no spaces in categories for regex
  mutate(
    I_heating = recode(I_heating, "No Info" = "No_Info")
    , I_cooling = recode(I_cooling, "No Info" = "No_Info")
    , I_parking = recode(I_parking, "No Info" = "No_Info")
    , I_outdoor = recode(I_outdoor, "No Info" = "No_Info") 
    , I_pool = recode(I_pool, "No Info" = "No_Info")
    , I_type = recode(I_type, "No Info" = "No_Info")
  )

########################
### descriptive stat ###
########################
df_summary_stat <- df[,1:25]
df_summary_stat <- df_summary_stat %>% mutate(
  time = as.duration(listed_date %--% sold_date)/ddays(1)
)
# jours (approximatif car *30)
summary(df_summary_stat$sale_time*30)
summary(df_summary_stat$event)

# setwd("results")
# if (!file.exists("plots")){dir.create("plots")}
# setwd(home_directory)
a <- ggplot(df, aes(x=sale_time)) + 
  geom_histogram(aes(y = ..density..), 
                 bins=20,
                 color="black", 
                 fill="gray") +
  theme_bw() +
  labs(x="Temps de vente des habitations (mois)", y="Proportion")
# a
# ggsave("results/plots/temps_vente_habitations_hist.png", a
#        ,scale=1
#        ,width=6
#        ,height=3)

# train-test-split
split <- train_valid_test(df, "event", p_train = 0.8, p_valid = 0.1)
train <- split$train$df
valid <- split$valid$df
test <- split$test$df

# After some thoughts, I don't think I will use a test set, so I will
# merge valid and test to make a bigger validation set set
test <- rbind(valid, test)
rm(valid)
```

## Models

```{r}
Formula <- as.formula(paste("Surv(sale_time, event) ~", paste("bbu_dim.", 1:num_dimension, collapse="+", sep="")))
# remark : doesn't work with 768

# model without regularization
text_pca_bbu <- coxph(
  Formula
  , data=train
  , x=TRUE # for glmnet
  , model=TRUE # for glmnet
)
summary(text_pca_bbu)
score_text_full_pca_bbu <- performance_measures(text_pca_bbu, train, test, surv_call="Surv(sale_time, event)")

# concordance index
text_cv_pca_bbu_Cindex <- cv.glmnet(x=text_pca_bbu$x, y=text_pca_bbu$y, family="cox"
               , nfolds = 5
               , type.measure="C"
               , alpha=0.95
               , trace.it = FALSE
               #, lambda.min.ratio = 0.1e-50
               )
coef(text_cv_pca_bbu_Cindex, s="lambda.1se")
text_cv_pca_bbu_Cindex %>% plot()

# loglik
text_cv_pca_bbu_loglik <- cv.glmnet(x=text_pca_bbu$x, y=text_pca_bbu$y, family="cox"
               , nfolds = 5
               , type.measure="default"
               , alpha=0.95
               , trace.it = FALSE
               #, lambda.min.ratio = 0.1e-50
               )
text_cv_pca_bbu_loglik %>% plot()

# Reevaluating models with coefficients
embedding_names <- paste("bbu_dim.", 1:num_dimension, collapse=",", sep="") %>%
  strsplit(., ",") %>%
  extract2(1) # extract first element of the list generated by strsplit
variables_names <- c("")

model_text_bbu_Cindex <- generate_newcox_glmnet(cvglmnet = text_cv_pca_bbu_Cindex
                      ,variables_names=variables_names
                      ,embedding_names=embedding_names
                      ,dataset=train
                      ,dataset_test = test
                      ,surv_call="Surv(sale_time, event)"
                      )$cox
model_text_bbu_Cindex %>% summary()

score_text_pca_bbu_regularized_Cindex <- performance_newcox_glmnet(cvglmnet = text_cv_pca_bbu_Cindex
                      ,variables_names=variables_names
                      ,embedding_names=embedding_names
                      ,dataset=train
                      ,dataset_test = test
                      ,surv_call="Surv(sale_time, event)"
                      )

score_text_pca_bbu_regularized_loglik <- performance_newcox_glmnet(cvglmnet = text_cv_pca_bbu_loglik
                      ,variables_names=variables_names
                      ,embedding_names=embedding_names
                      ,dataset=train
                      ,dataset_test = test
                      ,surv_call="Surv(sale_time, event)"
                      )

# for comparison, performance of the  model without regularization
scores_text_bbu = performance_measures(model=text_pca_bbu, olddata=train, newdata=test,surv_call="Surv(sale_time, event)") # takes more time to run, so in terms of performance/time, it might not be the best (same for fasttext without regularization). Instead of taking 1 minute, it takes several minutes.
# but what takes time is the bootstrap C-index, so maybe the full model isn't too bad.
```

## Scores

```{r}
score_text_full_pca_bbu # no regularization
score_text_pca_bbu_regularized_Cindex$scores
score_text_pca_bbu_regularized_Cindex$number_of_variables
score_text_pca_bbu_regularized_loglik$scores
score_text_pca_bbu_regularized_loglik$number_of_variables
```

# BERT with all the components necessary to explain 80\% of variability in the data (approx. 61 pc)

```{r}
# global variables
home_directory <- "~/trulia_project"
setwd(home_directory)

set.seed(1)

# R packages
library(tidyverse); library(magrittr); library(survival); library(survminer); library(lubridate); library(tm); library(FactoMineR); library(factoextra); library(feather); library(riskRegression); library(pec); library(survAUC); library(glmnet); library(plotmo)
# magrittr : use %$%
# lubridate : %--%
# tm : tf-idf
# FactoMineR : PCA
# pec : pec, crps
# survAUC : predErr
# plotmo : plot_glmnet

# My functions
source("utils.R")
source('generate_embeddings.R')
'%not in%' <- Negate('%in%')

# glmnet hyperparameters
glmnet.control(mxitnr=50)

############
# Datasets #
############

df_tfidf_99 <- read_feather("embeddings/tfidf_99.feather")
df_ft <- read_feather("embeddings/fasttext.feather")
df_bbu <- read_feather("embeddings/bert_base_uncased.feather")

# when I saved df_bbu, I hadn't modified sale_time > 20, so I do it here
# It is necessary to merge datasets properly.
df_bbu <- df_bbu %>%
  mutate(
    to_modify = case_when(sale_time > 20 ~ 1
                          ,TRUE ~ 0 # else
                          )
    , event = case_when(to_modify == 1 ~ 0 # censoring events flagged as to_modify
                       ,TRUE ~ event)
    , sale_time = case_when(to_modify ==1 ~ 20 # sale_time forced to 20 months
                            ,TRUE ~ sale_time)
  )

# PCA
# generating PCA coordinated for bert embeddings
pca_bbu <- df_bbu %>%
  select(V1:V768) %>%
  PCA(., scale.unit = FALSE, graph = F, ncp=768)

# using 80% rule (will be used in coxph later)
num_dimension <- length(as.data.frame(pca_bbu$eig)$'cumulative percentage of variance'[as.data.frame(pca_bbu$eig)$'cumulative percentage of variance' <= 80]) + 1 
#num_dimension = 300

pca_bbu_coord <- data.frame(pca_bbu$ind$coord)
colnames(pca_bbu_coord) <- paste0("bbu_dim.", 1:768)
df_bbu_pca <- cbind(df_bbu, pca_bbu_coord)

# merging df
# good practice : decide by which variable we merge df. It is not necessary here
# but I leave the code for future reference
# df <- left_join(df_tfidf_99, df_ft, by = c("sold_date", "sold_price", "listed_date", "listed_price", "year_built", "num_bath", "num_bed", "living_area", "lot_area", "I_heating", "I_cooling", "I_type", "subtype", "I_parking", "I_outdoor", "I_pool", "num_restaurants", "num_groceries", "num_nightlife", "description", "description_cased", "event", "price_ratio")) %>%
#   left_join(., df_bbu_pca, by = c("sold_date", "sold_price", "listed_date", "listed_price", "year_built", "num_bath", "num_bed", "living_area", "lot_area", "I_heating", "I_cooling", "I_type", "subtype", "I_parking", "I_outdoor", "I_pool", "num_restaurants", "num_groceries", "num_nightlife", "description", "description_cased", "event", "price_ratio"))

df <- left_join(df_tfidf_99, df_ft) %>%
  left_join(., df_bbu_pca)

# Houses on sale for >20 months or that took more than 20 months to sell
# are censored to 20 months
# THIS CODE IS NOT NECESSARY ANYMORE because I did it in 1_exploration and I
# exported the dataset.
# df <- df %>%
#   mutate(
#     to_modify = case_when(sale_time > 20 ~ 1
#                           ,TRUE ~ 0 # else
#                           )
#     , event = case_when(to_modify == 1 ~ 0 # censoring events flagged as to_modify
#                        ,TRUE ~ event)
#     , sale_time = case_when(to_modify ==1 ~ 20 # sale_time forced to 20 months
#                             ,TRUE ~ sale_time)
#   )
# before censoring events that took too long, there were 1567 events. After
# censoring at 20 months, there where 1537 events (20% of the dataset).

# REMARK : 
# For the function extract_nonzero_coeff to work, variables treated as
# factors must be transformed as factor here
df <- df %>%
  mutate(
    # format baths, beds, lot_area, etc
     year_built = as.numeric(year_built)
    ,num_bath = as.numeric(num_bath)
    ,num_bed = as.numeric(num_bed)
    ,living_area = as.numeric(living_area)
    ,lot_area = as.numeric(lot_area)
    ,listed_date = lubridate::date(listed_date)
    ,sold_date = lubridate::date(sold_date)
    ,I_heating = as.factor(I_heating)
    ,I_cooling = as.factor(I_cooling)
    ,I_parking = as.factor(I_parking)
    ,I_outdoor = as.factor(I_outdoor)
    ,I_pool = as.factor(I_pool)
    ,I_type = as.factor(I_type)
    ,event = as.numeric(event)
  ) %>%
  # need to have no spaces in categories for regex
  mutate(
    I_heating = recode(I_heating, "No Info" = "No_Info")
    , I_cooling = recode(I_cooling, "No Info" = "No_Info")
    , I_parking = recode(I_parking, "No Info" = "No_Info")
    , I_outdoor = recode(I_outdoor, "No Info" = "No_Info") 
    , I_pool = recode(I_pool, "No Info" = "No_Info")
    , I_type = recode(I_type, "No Info" = "No_Info")
  )

########################
### descriptive stat ###
########################
df_summary_stat <- df[,1:25]
df_summary_stat <- df_summary_stat %>% mutate(
  time = as.duration(listed_date %--% sold_date)/ddays(1)
)
# jours (approximatif car *30)
summary(df_summary_stat$sale_time*30)
summary(df_summary_stat$event)

# setwd("results")
# if (!file.exists("plots")){dir.create("plots")}
# setwd(home_directory)
a <- ggplot(df, aes(x=sale_time)) + 
  geom_histogram(aes(y = ..density..), 
                 bins=20,
                 color="black", 
                 fill="gray") +
  theme_bw() +
  labs(x="Temps de vente des habitations (mois)", y="Proportion")
# a
# ggsave("results/plots/temps_vente_habitations_hist.png", a
#        ,scale=1
#        ,width=6
#        ,height=3)

# train-test-split
split <- train_valid_test(df, "event", p_train = 0.8, p_valid = 0.1)
train <- split$train$df
valid <- split$valid$df
test <- split$test$df

# After some thoughts, I don't think I will use a test set, so I will
# merge valid and test to make a bigger validation set set
test <- rbind(valid, test)
rm(valid)
```

## Models

```{r}
Formula <- as.formula(paste("Surv(sale_time, event) ~", paste("bbu_dim.", 1:num_dimension, collapse="+", sep="")))
# remark : doesn't work with 768

# model without regularization
text_pca_bbu <- coxph(
  Formula
  , data=train
  , x=TRUE # for glmnet
  , model=TRUE # for glmnet
)
summary(text_pca_bbu)
score_text_full_pca_bbu <- performance_measures(text_pca_bbu, train, test, surv_call="Surv(sale_time, event)")

# concordance index
text_cv_pca_bbu_Cindex <- cv.glmnet(x=text_pca_bbu$x, y=text_pca_bbu$y, family="cox"
               , nfolds = 5
               , type.measure="C"
               , alpha=0.95
               , trace.it = FALSE
               #, lambda.min.ratio = 0.1e-50
               )
coef(text_cv_pca_bbu_Cindex, s="lambda.1se")
text_cv_pca_bbu_Cindex %>% plot()

# loglik
text_cv_pca_bbu_loglik <- cv.glmnet(x=text_pca_bbu$x, y=text_pca_bbu$y, family="cox"
               , nfolds = 5
               , type.measure="default"
               , alpha=0.95
               , trace.it = FALSE
               #, lambda.min.ratio = 0.1e-50
               )
text_cv_pca_bbu_loglik %>% plot()

# Reevaluating models with coefficients
embedding_names <- paste("bbu_dim.", 1:num_dimension, collapse=",", sep="") %>%
  strsplit(., ",") %>%
  extract2(1) # extract first element of the list generated by strsplit
variables_names <- c("")

model_text_bbu_Cindex <- generate_newcox_glmnet(cvglmnet = text_cv_pca_bbu_Cindex
                      ,variables_names=variables_names
                      ,embedding_names=embedding_names
                      ,dataset=train
                      ,dataset_test = test
                      ,surv_call="Surv(sale_time, event)"
                      )$cox
model_text_bbu_Cindex %>% summary()

score_text_pca_bbu_regularized_Cindex <- performance_newcox_glmnet(cvglmnet = text_cv_pca_bbu_Cindex
                      ,variables_names=variables_names
                      ,embedding_names=embedding_names
                      ,dataset=train
                      ,dataset_test = test
                      ,surv_call="Surv(sale_time, event)"
                      )

score_text_pca_bbu_regularized_loglik <- performance_newcox_glmnet(cvglmnet = text_cv_pca_bbu_loglik
                      ,variables_names=variables_names
                      ,embedding_names=embedding_names
                      ,dataset=train
                      ,dataset_test = test
                      ,surv_call="Surv(sale_time, event)"
                      )

# for comparison, performance of the  model without regularization
scores_text_bbu = performance_measures(model=text_pca_bbu, olddata=train, newdata=test,surv_call="Surv(sale_time, event)") # takes more time to run, so in terms of performance/time, it might not be the best (same for fasttext without regularization). Instead of taking 1 minute, it takes several minutes.
# but what takes time is the bootstrap C-index, so maybe the full model isn't too bad.
```

## Scores

```{r}
score_text_full_pca_bbu # no regularization
score_text_pca_bbu_regularized_Cindex$scores
score_text_pca_bbu_regularized_Cindex$number_of_variables
score_text_pca_bbu_regularized_loglik$scores
score_text_pca_bbu_regularized_loglik$number_of_variables
```